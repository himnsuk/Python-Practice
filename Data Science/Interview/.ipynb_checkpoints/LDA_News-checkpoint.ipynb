{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yviqYp8cBlAx"
   },
   "source": [
    "LINK => [LDA](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#:~:text=The%20most%20important%20tuning%20parameter,be%20%3E%201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR-Oh9kuCR9z"
   },
   "source": [
    "### Preprocessing\n",
    "Stop-word elimination: removal of the most common words in a language that are not helpful and in general unusable in text mining like prepositions, numbers, and words that do not contain applicable information for the study. In fact, in NLP, there is no particular general list of stop words used by all developers who choose their list based on their goal to improve the recommendation system performance.\n",
    "\n",
    "• Stemming: the conversion of words into their root, using stemming algorithms such as Snowball Stemmer.\n",
    "\n",
    "• Lemmatizing: used to enhance the system's accuracy by returning the base or dictionary form of a word.\n",
    "\n",
    "• Tokenizing: dividing a text input into tokens like phrases, words, or other meaningful elements (tokens). The outcome of tokenization is a sequence of tokens.\n",
    "\n",
    "• Identifying n-gram procedure such as bigram (phrases containing two words) and trigram (phrases containing three words) words and consider them as one word.\n",
    "\n",
    "After the preprocessing step, we applied a commonly used term-weighting method called TF-IDF, which is a pre-filtering stage with all the included TM methods. TF-IDF is a numerical statistic measure used to score the importance of a word (term) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF8GXy-IT097"
   },
   "source": [
    "## Description of Assumptions \n",
    "\n",
    "- Documents with similar topics will use similar group of words\n",
    "- Document Definition/Modeling\n",
    "  - Documents are probability distribution over latent topic\n",
    "  - Topics are probability distribution over words\n",
    "\n",
    "LDA takes a number of documents. It assumes that the words are in each document are related. It then tries to figure out the \"recipe\" for how each document could have been created. We just need to tell the model how many topics to construct and it uses that \"recipe\" to generate topic and word distributions over a corpus. Based on that output, we can identify similar documents within the corpus.\n",
    "\n",
    "\n",
    "**Advantages**\n",
    " - LDA is an effective tool for topic modeling.\n",
    " - Easy to understand conceptually\n",
    " - Has been shown to produce good results over many domains.\n",
    " - New application\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "-  Must know the number of topics K in advance\n",
    "- Dirichlet topic distribution cannot capture correlations among topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfiod9YJsPs_"
   },
   "source": [
    "### Hyperparameter Tuning in LDA\n",
    "First, let’s differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "__Model hyperparameters__ can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "__Model parameters__ can be thought of as what the model learns during training, such as the weights for each word in a given topic\n",
    "Now that we have the baseline coherence score for the default LDA model, let’s perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
    "\n",
    "1. Number of Topics (K)\n",
    "1. Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "1. Dirichlet hyperparameter beta: Word-Topic Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyQJzhN1_Jeq"
   },
   "source": [
    "## Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsqmMNjd9Awp",
    "outputId": "5a8790a6-b50c-437a-e44d-1b470b88f195"
   },
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jchjQP9QeGO2",
    "outputId": "b9f74c71-c6d6-4915-b2df-2bbea278d788"
   },
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccgQ7FUyscUz",
    "outputId": "26f2b1b5-0397-461a-c2c9-7a05dc56453a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-ySyIWhXeFCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtrYxGJG_Obl"
   },
   "source": [
    "## Import Newsgroups Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1SxzcoUsd8H",
    "outputId": "a4a93e38-b64a-44db-d4ff-5c6afe8b08bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "ruHONxXT9XqE",
    "outputId": "f3bc9386-ba4e-4ca8-b7b0-0f9424d9e89f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a4c36cfb-26a6-4086-9f06-e4b9c901edb7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>From: david@terminus.ericsson.se (David Bold)\\...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>From: rodc@fc.hp.com (Rod Cerkoney)\\nSubject: ...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>From: dbm0000@tm0006.lerc.nasa.gov (David B. M...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>From: jllee@acsu.buffalo.edu (Johnny L Lee)\\nS...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4c36cfb-26a6-4086-9f06-e4b9c901edb7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-a4c36cfb-26a6-4086-9f06-e4b9c901edb7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-a4c36cfb-26a6-4086-9f06-e4b9c901edb7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              content  target  \\\n",
       "0   From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1   From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2   From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3   From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4   From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "5   From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...      16   \n",
       "6   From: bmdelane@quads.uchicago.edu (brian manni...      13   \n",
       "7   From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...       3   \n",
       "8   From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...       2   \n",
       "9   From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...       4   \n",
       "10  From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "11  From: david@terminus.ericsson.se (David Bold)\\...      19   \n",
       "12  From: rodc@fc.hp.com (Rod Cerkoney)\\nSubject: ...       4   \n",
       "13  From: dbm0000@tm0006.lerc.nasa.gov (David B. M...      14   \n",
       "14  From: jllee@acsu.buffalo.edu (Johnny L Lee)\\nS...       6   \n",
       "\n",
       "                target_names  \n",
       "0                  rec.autos  \n",
       "1      comp.sys.mac.hardware  \n",
       "2      comp.sys.mac.hardware  \n",
       "3              comp.graphics  \n",
       "4                  sci.space  \n",
       "5         talk.politics.guns  \n",
       "6                    sci.med  \n",
       "7   comp.sys.ibm.pc.hardware  \n",
       "8    comp.os.ms-windows.misc  \n",
       "9      comp.sys.mac.hardware  \n",
       "10           rec.motorcycles  \n",
       "11        talk.religion.misc  \n",
       "12     comp.sys.mac.hardware  \n",
       "13                 sci.space  \n",
       "14              misc.forsale  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6ehprb0_S_v"
   },
   "source": [
    "## Remove emails and newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItfrOW_J9abe",
    "outputId": "94f23387-c599-4c4e-9546-3e94d96724d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-6-10af9153bd18>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "<ipython-input-6-10af9153bd18>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnpDcAq_aWL"
   },
   "source": [
    "## Tokenize and Clean-up using gensim’s simple_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAn_1mrO9e0N",
    "outputId": "099dda1a-4250-4bcd-959a-06ecc42131f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwBwldTj_ebi"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWJwBO0v9kst",
    "outputId": "9b9a0a0d-ea6f-4954-9512-23e62cc07fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s thing subject car nntp post host college park line wonder out there enlighten car see other day door sport car look late early call door really small addition front bumper separate rest body know tellme model name engine spec year production car make history info funky look car mail thank bring neighborhood lerxst', 'subject clock poll final call summary final call clock report keyword acceleration clock upgrade article line nntp post host fair number brave soul upgrade clock oscillator share experience poll send brief message detail experience procedure top speed attain cpu rate speed add card adapter heat sink hour usage day floppy disk functionality floppy especially request summarize next day so add network knowledge base do clock upgrade answer poll thank']\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_ybAsEEfe2Q",
    "outputId": "c8a2a7ef-4550-4205-8667-1fd0490ae5e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s thing subject car nntp post host college park line wonder out there enlighten car see other day door sport car look late early call door really small addition front bumper separate rest body know tellme model name engine spec year production car make history info funky look car mail thank bring neighborhood lerxst',\n",
       " 'subject clock poll final call summary final call clock report keyword acceleration clock upgrade article line nntp post host fair number brave soul upgrade clock oscillator share experience poll send brief message detail experience procedure top speed attain cpu rate speed add card adapter heat sink hour usage day floppy disk functionality floppy especially request summarize next day so add network knowledge base do clock upgrade answer poll thank']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MgZucGj_iNx"
   },
   "source": [
    "## Create the Document-Word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LiG4pYNJ9xKQ"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEzC-rz-_tLZ"
   },
   "source": [
    "## Check the Sparsicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vIZTFtd94T8",
    "outputId": "5701277e-4428-48e7-a267-195d5122f236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  0.8259859421715849 %\n"
     ]
    }
   ],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wsk9G3Z_wvT"
   },
   "source": [
    "## Build LDA model with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ra8PjA7R989B",
    "outputId": "894c0ed0-6528-470e-fcd6-b19d3ae56f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(learning_method='online', n_components=20, n_jobs=-1,\n",
      "                          random_state=100)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1DPlxUe-WPQ"
   },
   "source": [
    "#### Output of above code\n",
    "\n",
    "```python\n",
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7,\n",
    "             learning_method='online', learning_offset=10.0,\n",
    "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
    "             n_components=10, n_jobs=-1, n_topics=20, perp_tol=0.1,\n",
    "             random_state=100, topic_word_prior=None,\n",
    "             total_samples=1000000.0, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1FEW75b_1XJ"
   },
   "source": [
    "## Diagnose model performance with perplexity and log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAKFCLsT_2M8",
    "outputId": "993cf11c-5054-471f-d587-560154c3da08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -8153640.003229906\n",
      "Perplexity:  1721.0030040794254\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 20,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 100,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxX2WLOO_7dC"
   },
   "source": [
    "## How to GridSearch the best LDA model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3YnxG9uACBA"
   },
   "source": [
    "The most important tuning parameter for LDA models is n_components (number of topics). In addition, I am going to search learning_decay (which controls the learning rate) as well.\n",
    "\n",
    "Besides these, other possible search params could be learning_offset (downweigh early iterations. Should be > 1) and max_iter. These could be worth experimenting if you have enough computing resources.\n",
    "\n",
    "Be warned, the grid search constructs multiple LDA models for all possible combinations of param values in the param_grid dict. So, this process can consume a lot of time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zc5SsNN1_4az",
    "outputId": "72d522ac-01e1-48ca-9f90-c3837b2ff1dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
       "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
       "                         'n_components': [10, 15]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15], 'learning_decay': [.5, .7, .9]}\n",
    "# search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90GTXDrp0J8i"
   },
   "source": [
    "#### Output of above cell\n",
    "\n",
    "```python\n",
    "GridSearchCV(cv=None, error_score=nan,\n",
    "             estimator=LatentDirichletAllocation(batch_size=128,\n",
    "                                                 doc_topic_prior=None,\n",
    "                                                 evaluate_every=-1,\n",
    "                                                 learning_decay=0.7,\n",
    "                                                 learning_method='batch',\n",
    "                                                 learning_offset=10.0,\n",
    "                                                 max_doc_update_iter=100,\n",
    "                                                 max_iter=10,\n",
    "                                                 mean_change_tol=0.001,\n",
    "                                                 n_components=10, n_jobs=None,\n",
    "                                                 perp_tol=0.1,\n",
    "                                                 random_state=None,\n",
    "                                                 topic_word_prior=None,\n",
    "                                                 total_samples=1000000.0,\n",
    "                                                 verbose=0),\n",
    "             iid='deprecated', n_jobs=None,\n",
    "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
    "                         'n_components': [10, 15]},\n",
    "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "             scoring=None, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt4fGL_PAFTm"
   },
   "source": [
    "#### Output of the above cell\n",
    "\n",
    "```python\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZcbdbAYAJgk"
   },
   "source": [
    "## How to see the best topic model and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5fMBqtKALGe",
    "outputId": "844fedd6-511d-439f-d263-0b7abb0bad9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -1718429.7833504635\n",
      "Model Perplexity:  1623.2083379818696\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR9HKcfO1Pzg",
    "outputId": "722f46be-7ed7-46d4-dbfb-375a156535c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1718429.7833504635"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v1iFJwRAQYu"
   },
   "source": [
    "## Compare LDA Model Performance Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MuzmsVYAUKP"
   },
   "source": [
    "Plotting the log-likelihood scores against num_topics, clearly shows number of topics = 10 has better scores. And learning_decay of 0.7 outperforms both 0.5 and 0.9.\n",
    "\n",
    "This makes me think, even though we know that the dataset has 20 distinct topics to start with, some topics could share common keywords. For example, ‘alt.atheism’ and ‘soc.religion.christian’ can have a lot of common words. Same with ‘rec.motorcycles’ and ‘rec.autos’, ‘comp.sys.ibm.pc.hardware’ and ‘comp.sys.mac.hardware’, you get the idea.\n",
    "\n",
    "To tune this even further, you can do a finer grid search for number of topics between 10 and 15. But I am going to skip that for now.\n",
    "\n",
    "So the bottom line is, a lower optimal number of distinct topics (even 10 topics) may be reasonable for this dataset. I don’t know that yet. But LDA says so. Let’s see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkQERTks1_De",
    "outputId": "bc6b0700-d9c3-40eb-ce95-dfca47ac5f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([59.5211206 , 70.6821795 , 57.17428579, 68.53499546, 58.91123142,\n",
       "        69.47348566]),\n",
       " 'std_fit_time': array([1.16909945, 3.43938694, 1.11527192, 0.88950909, 1.22535391,\n",
       "        1.40551226]),\n",
       " 'mean_score_time': array([1.35757103, 1.41992121, 1.16648078, 1.45628572, 1.19785829,\n",
       "        1.46151004]),\n",
       " 'std_score_time': array([0.33321529, 0.0512939 , 0.05206522, 0.05800298, 0.03772571,\n",
       "        0.08270252]),\n",
       " 'param_learning_decay': masked_array(data=[0.5, 0.5, 0.7, 0.7, 0.9, 0.9],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_components': masked_array(data=[10, 15, 10, 15, 10, 15],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_decay': 0.5, 'n_components': 10},\n",
       "  {'learning_decay': 0.5, 'n_components': 15},\n",
       "  {'learning_decay': 0.7, 'n_components': 10},\n",
       "  {'learning_decay': 0.7, 'n_components': 15},\n",
       "  {'learning_decay': 0.9, 'n_components': 10},\n",
       "  {'learning_decay': 0.9, 'n_components': 15}],\n",
       " 'split0_test_score': array([-1661864.28936651, -1681440.45352782, -1661794.34965946,\n",
       "        -1680467.77684547, -1667521.24032579, -1681832.46554077]),\n",
       " 'split1_test_score': array([-1821173.09478781, -1839333.52668648, -1820550.24187333,\n",
       "        -1838078.27695413, -1821410.68938757, -1835496.07837538]),\n",
       " 'split2_test_score': array([-1738160.62309147, -1759126.7702443 , -1734442.5654753 ,\n",
       "        -1763485.16115413, -1739873.23404458, -1765449.89325174]),\n",
       " 'split3_test_score': array([-1699508.93309007, -1725588.93920468, -1698298.67027697,\n",
       "        -1720025.11298924, -1702172.78527575, -1719942.22784881]),\n",
       " 'split4_test_score': array([-1679003.29886908, -1694922.41882092, -1677063.08946725,\n",
       "        -1697303.08884581, -1677818.40582847, -1698610.16792677]),\n",
       " 'mean_test_score': array([-1719942.04784099, -1740082.42169684, -1718429.78335046,\n",
       "        -1739871.88335775, -1721759.27097243, -1740266.1665887 ]),\n",
       " 'std_test_score': array([56650.08512851, 56394.74510715, 56584.51142508, 56456.93413971,\n",
       "        55704.06604602, 55255.29534103]),\n",
       " 'rank_test_score': array([2, 5, 1, 4, 3, 6], dtype=int32)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "01DSoiFA6Jc0",
    "outputId": "bc3920cf-47c7-4944-f74c-e593b7c99aa6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-41313928-4028-469b-85ae-4d97730be1b5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_decay</th>\n",
       "      <th>param_n_components</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.521121</td>\n",
       "      <td>1.169099</td>\n",
       "      <td>1.357571</td>\n",
       "      <td>0.333215</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 10}</td>\n",
       "      <td>-1.661864e+06</td>\n",
       "      <td>-1.821173e+06</td>\n",
       "      <td>-1.738161e+06</td>\n",
       "      <td>-1.699509e+06</td>\n",
       "      <td>-1.679003e+06</td>\n",
       "      <td>-1.719942e+06</td>\n",
       "      <td>56650.085129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.682179</td>\n",
       "      <td>3.439387</td>\n",
       "      <td>1.419921</td>\n",
       "      <td>0.051294</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.5, 'n_components': 15}</td>\n",
       "      <td>-1.681440e+06</td>\n",
       "      <td>-1.839334e+06</td>\n",
       "      <td>-1.759127e+06</td>\n",
       "      <td>-1.725589e+06</td>\n",
       "      <td>-1.694922e+06</td>\n",
       "      <td>-1.740082e+06</td>\n",
       "      <td>56394.745107</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.174286</td>\n",
       "      <td>1.115272</td>\n",
       "      <td>1.166481</td>\n",
       "      <td>0.052065</td>\n",
       "      <td>0.7</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 10}</td>\n",
       "      <td>-1.661794e+06</td>\n",
       "      <td>-1.820550e+06</td>\n",
       "      <td>-1.734443e+06</td>\n",
       "      <td>-1.698299e+06</td>\n",
       "      <td>-1.677063e+06</td>\n",
       "      <td>-1.718430e+06</td>\n",
       "      <td>56584.511425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.534995</td>\n",
       "      <td>0.889509</td>\n",
       "      <td>1.456286</td>\n",
       "      <td>0.058003</td>\n",
       "      <td>0.7</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.7, 'n_components': 15}</td>\n",
       "      <td>-1.680468e+06</td>\n",
       "      <td>-1.838078e+06</td>\n",
       "      <td>-1.763485e+06</td>\n",
       "      <td>-1.720025e+06</td>\n",
       "      <td>-1.697303e+06</td>\n",
       "      <td>-1.739872e+06</td>\n",
       "      <td>56456.934140</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.911231</td>\n",
       "      <td>1.225354</td>\n",
       "      <td>1.197858</td>\n",
       "      <td>0.037726</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 10}</td>\n",
       "      <td>-1.667521e+06</td>\n",
       "      <td>-1.821411e+06</td>\n",
       "      <td>-1.739873e+06</td>\n",
       "      <td>-1.702173e+06</td>\n",
       "      <td>-1.677818e+06</td>\n",
       "      <td>-1.721759e+06</td>\n",
       "      <td>55704.066046</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.473486</td>\n",
       "      <td>1.405512</td>\n",
       "      <td>1.461510</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.9</td>\n",
       "      <td>15</td>\n",
       "      <td>{'learning_decay': 0.9, 'n_components': 15}</td>\n",
       "      <td>-1.681832e+06</td>\n",
       "      <td>-1.835496e+06</td>\n",
       "      <td>-1.765450e+06</td>\n",
       "      <td>-1.719942e+06</td>\n",
       "      <td>-1.698610e+06</td>\n",
       "      <td>-1.740266e+06</td>\n",
       "      <td>55255.295341</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41313928-4028-469b-85ae-4d97730be1b5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-41313928-4028-469b-85ae-4d97730be1b5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-41313928-4028-469b-85ae-4d97730be1b5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      59.521121      1.169099         1.357571        0.333215   \n",
       "1      70.682179      3.439387         1.419921        0.051294   \n",
       "2      57.174286      1.115272         1.166481        0.052065   \n",
       "3      68.534995      0.889509         1.456286        0.058003   \n",
       "4      58.911231      1.225354         1.197858        0.037726   \n",
       "5      69.473486      1.405512         1.461510        0.082703   \n",
       "\n",
       "  param_learning_decay param_n_components  \\\n",
       "0                  0.5                 10   \n",
       "1                  0.5                 15   \n",
       "2                  0.7                 10   \n",
       "3                  0.7                 15   \n",
       "4                  0.9                 10   \n",
       "5                  0.9                 15   \n",
       "\n",
       "                                        params  split0_test_score  \\\n",
       "0  {'learning_decay': 0.5, 'n_components': 10}      -1.661864e+06   \n",
       "1  {'learning_decay': 0.5, 'n_components': 15}      -1.681440e+06   \n",
       "2  {'learning_decay': 0.7, 'n_components': 10}      -1.661794e+06   \n",
       "3  {'learning_decay': 0.7, 'n_components': 15}      -1.680468e+06   \n",
       "4  {'learning_decay': 0.9, 'n_components': 10}      -1.667521e+06   \n",
       "5  {'learning_decay': 0.9, 'n_components': 15}      -1.681832e+06   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0      -1.821173e+06      -1.738161e+06      -1.699509e+06      -1.679003e+06   \n",
       "1      -1.839334e+06      -1.759127e+06      -1.725589e+06      -1.694922e+06   \n",
       "2      -1.820550e+06      -1.734443e+06      -1.698299e+06      -1.677063e+06   \n",
       "3      -1.838078e+06      -1.763485e+06      -1.720025e+06      -1.697303e+06   \n",
       "4      -1.821411e+06      -1.739873e+06      -1.702173e+06      -1.677818e+06   \n",
       "5      -1.835496e+06      -1.765450e+06      -1.719942e+06      -1.698610e+06   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0    -1.719942e+06    56650.085129                2  \n",
       "1    -1.740082e+06    56394.745107                5  \n",
       "2    -1.718430e+06    56584.511425                1  \n",
       "3    -1.739872e+06    56456.934140                4  \n",
       "4    -1.721759e+06    55704.066046                3  \n",
       "5    -1.740266e+06    55255.295341                6  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv_model_result = pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ygzzPWtp6w4n",
    "outputId": "c7d89b70-2d4c-418e-c51e-a8db621f8fff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.719942e+06\n",
       "1   -1.740082e+06\n",
       "2   -1.718430e+06\n",
       "3   -1.739872e+06\n",
       "4   -1.721759e+06\n",
       "5   -1.740266e+06\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv_model_result.columns\n",
    "grid_cv_model_result[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "Cq77Z_14ANsx",
    "outputId": "55075c6a-edf1-47d5-82d2-9f6c922cf5bc"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-84537683e6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Log Likelyhoods from Grid Search Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_likelyhoods_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlog_likelyhoods_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_likelyhoods_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-84537683e6f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Log Likelyhoods from Grid Search Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_likelyhoods_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlog_likelyhoods_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_likelyhoods_9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.cv_results_[\"params\"] if gscore.params['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.cv_results_[\"params\"] if gscore.params['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.cv_results_[\"params\"] if gscore.params['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ulwqR3VAZ8b"
   },
   "source": [
    "## How to see the dominant topic in each document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTqa8ylXAgIS"
   },
   "source": [
    "To classify a document as belonging to a particular topic, a logical approach is to see which topic has the highest contribution to that document and assign it.\n",
    "In the table below, I’ve greened out all major topics in a document and assigned the most dominant topic in its own column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNkZULyoAW7Z"
   },
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxFHVAl5AlP2"
   },
   "source": [
    "## Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYa3UmwYAjJS"
   },
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQLZXoM4Aq5C"
   },
   "source": [
    "## How to visualize the LDA model with pyLDAvis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vheJqRsyAob2"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5ujEPEAvkA"
   },
   "source": [
    "## How to see the Topic’s keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9B_xiCcA0RB"
   },
   "source": [
    "The weights of each keyword in each topic is contained in lda_model.components_ as a 2d array. The names of the keywords itself can be obtained from vectorizer object using get_feature_names().\n",
    "\n",
    "Let’s use this info to construct a weight matrix for all keywords in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w8MEpUjAvWV"
   },
   "outputs": [],
   "source": [
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-MDOBs3A3mX"
   },
   "source": [
    "## Get the top 15 keywords each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g98cTR75A3QG"
   },
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLyp-nJtA_I5"
   },
   "source": [
    "## How to predict the topics for a new piece of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQPhtJEBDWo"
   },
   "source": [
    "Assuming that you have already built the topic model, you need to take the text through the same routine of transformations and before predicting the topic.\n",
    "\n",
    "For our case, the order of transformations is:\n",
    "\n",
    "`sent_to_words() –> lemmatization() –> vectorizer.transform() –> best_lda_model.transform()`\n",
    "\n",
    "You need to apply these transformations in the same order. So to simplify it, let’s combine these steps into a predict_topic() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnsxEt4SA8ye"
   },
   "outputs": [],
   "source": [
    "def predict_topic(text, nlp=nlp):\n",
    "    global sent_to_words\n",
    "    global lemmatization\n",
    "\n",
    "    # Step 1: Clean with simple_preprocess\n",
    "    mytext_2 = list(sent_to_words(text))\n",
    "\n",
    "    # Step 2: Lemmatize\n",
    "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Step 3: Vectorize transform\n",
    "    mytext_4 = vectorizer.transform(mytext_3)\n",
    "\n",
    "    # Step 4: LDA Transform\n",
    "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
    "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
    "    return topic, topic_probability_scores\n",
    "\n",
    "# Predict the topic\n",
    "mytext = [\"Some text about christianity and bible\"]\n",
    "topic, prob_scores = predict_topic(text = mytext)\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5YToPBh2H0R"
   },
   "outputs": [],
   "source": [
    "print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Q9Z44WBOA8"
   },
   "source": [
    "## How to cluster documents that share similar topics and plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "227SEuKgBPra"
   },
   "source": [
    "You can use k-means clustering on the document-topic probabilioty matrix, which is nothing but lda_output object. Since out best model has 15 clusters, I’ve set n_clusters=15 in KMeans().\n",
    "\n",
    "Alternately, you could avoid k-means and instead, assign the cluster as the topic column number with the highest probability score.\n",
    "\n",
    "We now have the cluster number. But we also need the X and Y columns to draw the plot.\n",
    "\n",
    "For the X and Y, you can use SVD on the lda_output object with n_components as 2. SVD ensures that these two columns captures the maximum possible amount of information from lda_output in the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeUgcDWRBTz5"
   },
   "outputs": [],
   "source": [
    "# Construct the k-means clusters\n",
    "from sklearn.cluster import KMeans\n",
    "clusters = KMeans(n_clusters=15, random_state=100).fit_predict(lda_output)\n",
    "\n",
    "# Build the Singular Value Decomposition(SVD) model\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "\n",
    "# Weights for the 15 columns of lda_output, for each component\n",
    "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
    "\n",
    "# Percentage of total information in 'lda_output' explained by the two components\n",
    "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axGFJ4_KBZXS"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y, c=clusters)\n",
    "plt.xlabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title(\"Segregation of Topic Clusters\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAUtZTv3BcZi"
   },
   "source": [
    "## How to get similar documents for any given piece of text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6KBbwi4BdLc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def similar_documents(text, doc_topic_probs, documents = data, nlp=nlp, top_n=5, verbose=False):\n",
    "    topic, x  = predict_topic(text)\n",
    "    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
    "    doc_ids = np.argsort(dists)[:top_n]\n",
    "    if verbose:        \n",
    "        print(\"Topic KeyWords: \", topic)\n",
    "        print(\"Topic Prob Scores of text: \", np.round(x, 1))\n",
    "        print(\"Most Similar Doc's Probs:  \", np.round(doc_topic_probs[doc_ids], 1))\n",
    "    return doc_ids, np.take(documents, doc_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yANnUf_vBhC_"
   },
   "outputs": [],
   "source": [
    "# Get similar documents\n",
    "mytext = [\"Some text about christianity and bible\"]\n",
    "doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = data, top_n=1, verbose=True)\n",
    "print('\\n', docs[0][:500])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1MgZucGj_iNx",
    "MEzC-rz-_tLZ",
    "7Wsk9G3Z_wvT",
    "u1DPlxUe-WPQ",
    "90GTXDrp0J8i",
    "nt4fGL_PAFTm",
    "jZcbdbAYAJgk",
    "7v1iFJwRAQYu",
    "7ulwqR3VAZ8b",
    "qxFHVAl5AlP2",
    "XQLZXoM4Aq5C",
    "JN5ujEPEAvkA",
    "4-MDOBs3A3mX",
    "NLyp-nJtA_I5",
    "A1Q9Z44WBOA8",
    "dAUtZTv3BcZi"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
